### umosmots

Public repository for our WACV2023 Accepted Paper: "Unsupervised 4D LiDAR Moving Object Segmentation in Stationary Settings
with Multivariate Occupancy Time Series" by Thomas Kreutz, Max Mühlhäuser, and Alejandro Sanchez Guinea - [paper](https://openaccess.thecvf.com/content/WACV2023/html/Kreutz_Unsupervised_4D_LiDAR_Moving_Object_Segmentation_in_Stationary_Settings_With_WACV_2023_paper.html) - [arxiv](https://arxiv.org/abs/2212.14750)

### Abstract
In this work, we address the problem of unsupervised moving object segmentation (MOS) in 4D LiDAR data recorded from a stationary sensor, where no ground truth annotations are involved. Deep learning-based state-of-the-art methods for LiDAR MOS strongly depend on annotated ground truth data, which is expensive to obtain and scarce in existence.
To close this gap in the stationary setting, we propose a novel 4D LiDAR representation based on multivariate time series that relaxes the problem of unsupervised MOS to a time series clustering problem. More specifically, we propose modeling the change in occupancy of a voxel by a multivariate occupancy time series (MOTS), which captures spatio-temporal occupancy changes on the voxel level and its surrounding neighborhood. To perform unsupervised MOS, we train a neural network in a self-supervised manner to encode MOTS into voxel-level feature representations, which can be partitioned by a clustering algorithm into moving or stationary. Experiments on stationary scenes from the Raw KITTI dataset show that our fully unsupervised approach achieves performance that is comparable to that of supervised state-of-the-art approaches.

![alt text](https://github.com/thkreutz/umosmots/blob/main/visualization/demo.gif)

### Dependencies and Installation
- check Requirements.txt 
- (note about ME: only used for voxelization, so CPU version is enough.)

### Data

Step 1: Get SemanticKITTI training set (only the training set is needed) (http://semantic-kitti.org/)

Step 2: Download respective Raw KITTI sequences from https://www.cvlibs.net/datasets/kitti/raw_data.php

| Name KITTI | Name in paper | Seq ID |
| ---  | --- | --- |
| 2011 09 26 drive 0017 sync (City category) | City1 | 22 |
| 2011 09 26 drive 0060 sync (City category) | City2 | 23 |
| 2011 09 28 drive 0016 sync (Campus category) | Campus1 | 24 |

Step 3: Extract them, rename each sequence to Seq ID, mkdir data/rawkitti, and move to data/rawkitti. You may also extract them into SemanticKITTI/sequences/ and obtain SemanticKITTI/sequences/<22,23,24>

Step 4: Now, we add our provided labels (and generated pose+calib files)

- Unzip wacv_labels.zip into data/rawkitti/
- wacv_labels.zip includes <22,23,24>/labels/<...> , <22,23,24>/poses.txt, <22,23,24>/calib.txt (and notes that just tells your which scene it is, and instances.txt, which is just autogenerated)
- Merge with your <22,23,24> folders you obtained in the previous step.
- The scenes have been annotated using https://github.com/jbehley/point_labeler/ 
- Pose and calib files created with https://github.com/jbehley/point_labeler/blob/master/scripts/kitti_raw2odometry.py


### Preprocess Data (for Training only)
For training, we use a preprocesssing script that pre-saves sliding windows and their voxelization.

```
python tk_umos_preprocess_script.py -o data/SlidingWindowsPreprocessed -d semantic_kitti -dp data/SemanticKITTI/sequences -w 20 -v 0.1
```

- -d semantic_kitti specifies that SemanticKITTI dataset is used.
- -o data/SlidingWindowsPreprocessed , saves data to data/SlidingWindowsPreprocessed/semantic_kitti-20-01/...
- -dp path to your SemanticKITTI/sequences directory, preprocesses scene 00-10 (without validation sequence). 
- Specifiy w=window size, v=voxel size


### Training a model

```
python tk_umos_train_sparse.py -o models/ --dataset data/SlidingWindowsPreprocessed/semantic_kitti-20-01 -r 2 -w 20 -ed 32 --epochs 2 -b 1024
```

- output path -o models/ 
- --dataset , path to directory of the preprocessed windows from previous step
- specify radius r, window size w (must match preprocessed), embedding dimension ed, epochs, batch_size

Model is saved to models/<generated mid>

### Pre-trained model
pre-trained model is provided in models/1661045943760
- see evaluation section how to evaluate
- visualization -> Visualization.ipynb


### Evaluation

```
python tk_umos_evaluation_script.py -d data/rawkitti/ -seq 22 -mp models/ -mid 1661045943760
```

- -d path to kitti sequence directory 
- -seq specify the sequence (22,23, or 24 in our case)
- -mp models directory (default is models/)
- -mid id of your model directory. If it is models/ -> models/<mid>.
- hardcoded to "checkpoint_1.pth.tar"
- Will save the gmm model, predictions, mapped_predictions, and a list of mIoU's for each frame into models/predictions/<seq_id>
- Prints mIoU results and other information in terminal.
- Check Visualization.ipynb notebook how to use them.

### Visualization
Check Visualization.ipynb notebook.




### VLP16 Data
- link coming soon


### Some Links
SemanticKITTI http://semantic-kitti.org/

KITTI https://www.cvlibs.net/datasets/kitti/

Distinctipy https://github.com/alan-turing-institute/distinctipy


