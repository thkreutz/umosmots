{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e91fafd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T11:13:00.490285Z",
     "start_time": "2023-05-13T11:13:00.483387Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import open3d as o3d\n",
    "from src.data import tk_data\n",
    "import numpy as np\n",
    "import MinkowskiEngine as ME\n",
    "import torch\n",
    "\n",
    "from tk_umos_publication_evaluation_script import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1744a5ce",
   "metadata": {},
   "source": [
    "### Information about paths, args, and which model to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee8997f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T08:54:49.086667Z",
     "start_time": "2023-05-10T08:54:49.005474Z"
    }
   },
   "outputs": [],
   "source": [
    "data_root_path = \"/workspace/approaches/WACV_UMOS/data\" ## TO_MODIFY: Modify this to data root path\n",
    "model_root_path = \"/workspace/approaches/WACV_UMOS/models\"  ## TO_MODIFY: Modify this to your model root path\n",
    "\n",
    "scenes = {\"Campus1\" : \"22\",\n",
    "          \"City1\" :  \"23\",\n",
    "           \"City2\" : \"24\"}\n",
    "\n",
    "eval_scene = \"City1\"\n",
    "eval_scene = scenes[eval_scene]\n",
    "scene_path = os.path.join(data_root_path, eval_scene)\n",
    "window_size = 20\n",
    "\n",
    "# prediction\n",
    "model = \"1683644585490\"\n",
    "n_clusters = 10\n",
    "preds = sorted(glob.glob(\"models/%s/predictions/%s/k%s-0/raw_*.npy\" % (model, eval_scene, n_clusters)))\n",
    "mapped_preds = sorted(glob.glob(\"models/%s/predictions/%s/k%s-0/mapped_*.npy\" % (model, eval_scene, n_clusters)))\n",
    "predictions = [np.load(p) for p in preds] \n",
    "mapped_predictions = [np.load(p) for p in mapped_preds] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10585ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T08:43:42.689017Z",
     "start_time": "2023-05-10T08:43:42.673663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/approaches/WACV_UMOS/data/23'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de846a",
   "metadata": {},
   "source": [
    "### Read the data and the ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c85b0c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T08:44:14.764135Z",
     "start_time": "2023-05-10T08:43:44.975870Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:00, 214.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Read scans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114it [00:00, 209.67it/s]\n",
      "46it [00:00, 458.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Ego-motion compensation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114it [00:00, 352.61it/s]\n",
      "  9%|▉         | 9/95 [00:00<00:00, 88.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing frames to positive voxel coordinates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:12<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting voxels and labels to allign with MOTS predictions\n",
      "Create sliding windows...\n"
     ]
    }
   ],
   "source": [
    "_, pcs, labels = load_data(scene_path, window_size=window_size, voxel_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28a0cd",
   "metadata": {},
   "source": [
    "### Visualize RAW Clustering\n",
    "\n",
    "An open3d window will open on your screen.\n",
    "\n",
    "Navigate forward or backward using left or right arrow, close using ESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eaf49802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T11:19:19.089111Z",
     "start_time": "2023-05-09T11:19:18.918531Z"
    }
   },
   "outputs": [],
   "source": [
    "import distinctipy\n",
    "# get colors for the number of clusters\n",
    "colors = distinctipy.get_colors(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c19ac06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T11:13:55.107252Z",
     "start_time": "2023-05-09T11:13:50.133661Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [00:04, 19.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# assign colors to each voxel in the pointcloud\n",
    "seq = []\n",
    "for pc, lbl in tqdm(zip(pcs, predictions)):  \n",
    "    pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(pc))\n",
    "    pcd.colors = o3d.utility.Vector3dVector([colors[cluster] for cluster in lbl])\n",
    "    seq.append(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_data.vis_sequence(seq, name=\"visualization/demo\", capture=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0e07e",
   "metadata": {},
   "source": [
    "### Visualize Ground Truth or Mapped Predictions\n",
    "\n",
    "An open3d window will open on your screen.\n",
    "\n",
    "Navigate forward or backward using left or right arrow, close using ESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3123b3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T08:44:30.008212Z",
     "start_time": "2023-05-10T08:44:29.999849Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# fg and bg color\n",
    "bg_color = (0.8, 0.8, 0.8)\n",
    "fg_color = (0.2, 0.2, 0.7)\n",
    "seq = []\n",
    "\n",
    "#for pc, lbl in tqdm(zip(pcs, labels)):  \n",
    "for pc, lbl in tqdm(zip(pcs, mapped_predictions)):  \n",
    "    pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(pc))\n",
    "    pcd.colors = o3d.utility.Vector3dVector([fg_color if l==1 else bg_color for l in lbl])\n",
    "    seq.append(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23894e4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T08:36:58.566103Z",
     "start_time": "2023-05-10T08:36:29.840970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_frames=167\n",
      "end reached, restarting\n",
      "end reached, restarting\n",
      "end reached, restarting\n",
      "end reached, restarting\n"
     ]
    }
   ],
   "source": [
    "# Navigate forward or backward using left or right arrow, close using ESC\n",
    "tk_data.vis_sequence(seq, name=\"visualization/demo\", capture=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96e1f1",
   "metadata": {},
   "source": [
    "### Save as GIF\n",
    "\n",
    "set capture=True and your respective output path. It will save one image for each frame. \n",
    "\n",
    "-> visualization/demo_1.png, ... , visualization/demo_N.png\n",
    "\n",
    "Afterward, use the script \"make_vid.sh\", which will take the path to the files as an input, and will generate a gif in the same path.\n",
    "\n",
    "./make_vid.sh visualization/demo\n",
    "\n",
    "creates visualization/demo.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5398358f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T12:04:04.621748Z",
     "start_time": "2023-05-13T12:04:00.266451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_frames=167\n"
     ]
    }
   ],
   "source": [
    "### Save frames to make GIF -> capture=True \n",
    "# -> is laggy, but just step through the sequence once, it will save one figure for each frame\n",
    "tk_data.vis_sequence(seq, name=\"visualization/demo\", capture=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8931bec9",
   "metadata": {},
   "source": [
    "### Get predictions from data, model and clustering.\n",
    "\n",
    "This section serves as an example on how to obtain predictions. We will load the gmm and model that achieved the best results in our publication in this example. The code is almost identical to the evaluation script.\n",
    "\n",
    "Using this example, you should be able to adapt it and obtain predictions for any LiDAR dataset of your choice.\n",
    "\n",
    "If you do not have any labels, adapt the \"load_data\" part accordingly to not load any labels (dataloader has an option with_labels=False) and just obtain the voxelized sliding windows. Just study the \"load_data\" method a bit and you just understand easily. :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c917248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T11:11:11.896166Z",
     "start_time": "2023-05-13T11:11:11.886206Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "scene_path = \"data/rawkitti/22/\"\n",
    "window_size = 20\n",
    "start_frame = 0\n",
    "end_frame = -1\n",
    "radius = 2\n",
    "embedding_dim = 32\n",
    "n_clusters = 20\n",
    "\n",
    "model_id = \"1661045943760\"\n",
    "#gmm = joblib.load(\"models/%s/predictions/22/k20-0/gmm.joblib\" % model_id) #-> how to load from predictions folder.\n",
    "gmm = joblib.load(\"models/%s/gmms/gmm_22.joblib\"%model_id) # load publication gmm for scene 22\n",
    "checkpoint_path = \"models/%s/checkpoint_1.pth.tar\" % model_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02e16891",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T11:14:00.373886Z",
     "start_time": "2023-05-13T11:13:47.398895Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:00, 178.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and preparing data....\n",
      "1. Read scans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186it [00:00, 392.97it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Ego-motion compensation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186it [00:01, 117.06it/s]\n",
      "  3%|▎         | 5/167 [00:00<00:03, 48.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing frames to positive voxel coordinates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [00:03<00:00, 48.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting voxels and labels to allign with MOTS predictions\n",
      "Create sliding windows...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.umos_ae import UMOSAE\n",
    "from src.data.MOTS_Datasets import MOTS_Dataset\n",
    "\n",
    "print(\"Reading and preparing data....\")\n",
    "# reads sliding window frames and voxelizes them\n",
    "frames, vs, labels = load_data(scene_path, window_size=window_size, start_frame=start_frame, end_frame=end_frame)\n",
    "dataset = MOTS_Datasets.MOTS_Dataset(frames, r=radius, window_size=window_size)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = UMOSAE(input_channels=dataset.mots_utils.d.shape[0], window_size=window_size, embedding_dim=embedding_dim).cuda()\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "611e1db0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T11:21:32.491292Z",
     "start_time": "2023-05-13T11:16:43.579459Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [04:48<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# Now that we have loaded the model and data, we make our predictions.\n",
    "# First encode all mots\n",
    "embs = []\n",
    "voxels_frame = []\n",
    "for i in tqdm(range(len(frames))):\n",
    "    with torch.no_grad():\n",
    "        features, vxls = dataset[i] # one batch == all MOTS of a single frame -> shape (n_voxels, n_channels, window_size)\n",
    "        # voxels in here correspond to the voxels in vs, they are not ordered the same though.\n",
    "        embs.append(model.projection_head(torch.flatten(model.encoder(features.cuda()), start_dim=1)).cpu().detach().numpy())\n",
    "        voxels_frame.append(vxls)\n",
    "\n",
    "# -> Train clustering model here. In this example, we used a pre-trained one.\n",
    "# clustering = ClusterModel(...).fit(np.vstack(embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "705c2582",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T11:25:07.761217Z",
     "start_time": "2023-05-13T11:25:07.369669Z"
    }
   },
   "outputs": [],
   "source": [
    "import distinctipy\n",
    "n_clusters = 20\n",
    "colors = distinctipy.get_colors(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc16cb28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T11:29:48.651968Z",
     "start_time": "2023-05-13T11:25:18.152531Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [04:30,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# Afterward, prediction + visualization.\n",
    "seq = []\n",
    "for emb, pc in tqdm(zip(embs, voxels_frame)):  # -> window size not alligned\n",
    "    pcd = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(pc))\n",
    "    preds = gmm.predict(emb)\n",
    "    pcd.colors = o3d.utility.Vector3dVector([colors[cluster] for cluster in preds])\n",
    "    seq.append(pcd)\n",
    "\n",
    "# The clustering model partitions our data into clusters, which reflect individual structures identified within the data. \n",
    "# -> Since the approach is not supervised, the the approach does not know what these structures are, it just finds them.\n",
    "# To give an interpretation to the clusters, you can\n",
    "# a) use labels\n",
    "# b) do it manually, i.e., identify yourself which clusters belong to moving by watching a video of the raw clustering predictions.\n",
    "# c) Implement an automated method that includes some domain knowledge about the structure you expect in a cluster.\n",
    "#        -> e.g., instance segmentation+tracking => traces, shape consistency, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9127a215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T11:30:32.183450Z",
     "start_time": "2023-05-13T11:30:07.070578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_frames=167\n",
      "end reached, restarting\n"
     ]
    }
   ],
   "source": [
    "tk_data.vis_sequence(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e019769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
